#!/usr/bin/env python3
"""
Memory Efficiency Demonstration for JaxARC

This example demonstrates the memory optimization features implemented in JaxARC,
focusing on format-specific action history storage and efficient serialization.
It shows how different action formats (point, bbox, mask) use dramatically
different amounts of memory.

Key Features Demonstrated:
- Format-specific action history memory usage
- Memory usage comparison across action formats
- Efficient serialization with task_data exclusion
- Memory profiling and optimization strategies
- Best practices for memory-efficient usage

Usage:
    pixi run python examples/advanced/memory_efficiency_demo.py
"""

import os
import tempfile
import time
from typing import Dict, List, Tuple

import equinox as eqx
import jax
import jax.numpy as jnp
from loguru import logger

from jaxarc.envs import JaxArcConfig, EnvironmentConfig, UnifiedDatasetConfig, UnifiedActionConfig
from jaxarc.envs.functional import arc_reset, arc_step
from jaxarc.envs.structured_actions import PointAction, BboxAction, MaskAction
from jaxarc.parsers import ArcAgiParser
from jaxarc.state import ArcEnvState
from jaxarc.utils.jax_types import PRNGKey


def create_test_config() -> JaxArcConfig:
    """Create a test configuration."""
    return JaxArcConfig(
        environment=EnvironmentConfig(max_episode_steps=100),
        dataset=UnifiedDatasetConfig(max_grid_height=30, max_grid_width=30),
        action=UnifiedActionConfig(selection_format="point")
    )


def analyze_action_format_memory():
    """Analyze memory usage for different action formats."""
    logger.info("üíæ Action Format Memory Analysis")
    
    # Create configurations for different action formats
    configs = {}
    
    # Point format configuration
    point_config = create_test_config()
    point_config = eqx.tree_at(
        lambda c: c.action.selection_format,
        point_config,
        "point"
    )
    configs['point'] = point_config
    
    # Bbox format configuration
    bbox_config = create_test_config()
    bbox_config = eqx.tree_at(
        lambda c: c.action.selection_format,
        bbox_config,
        "bbox"
    )
    configs['bbox'] = bbox_config
    
    # Mask format configuration
    mask_config = create_test_config()
    mask_config = eqx.tree_at(
        lambda c: c.action.selection_format,
        mask_config,
        "mask"
    )
    configs['mask'] = mask_config
    
    # Setup task
    parser = ArcAgiParser()
    task = parser.get_task_by_index(0)
    key = jax.random.PRNGKey(42)
    
    memory_analysis = {}
    
    for format_name, config in configs.items():
        logger.info(f"  Analyzing {format_name} format...")
        
        # Create environment state
        state, obs = arc_reset(key, config, task)
        
        # Analyze memory usage
        action_history_size = state.action_history.nbytes
        action_history_shape = state.action_history.shape
        
        # Calculate total state memory
        total_memory = 0
        field_memories = {}
        
        for field_name in state.__dict__:
            if hasattr(state, field_name):
                field_value = getattr(state, field_name)
                if hasattr(field_value, 'nbytes'):
                    field_memory = field_value.nbytes
                    field_memories[field_name] = field_memory
                    total_memory += field_memory
        
        # Store results
        memory_analysis[format_name] = {
            'action_history_bytes': action_history_size,
            'action_history_shape': action_history_shape,
            'action_history_mb': action_history_size / (1024 * 1024),
            'total_memory_bytes': total_memory,
            'total_memory_mb': total_memory / (1024 * 1024),
            'field_memories': field_memories
        }
        
        logger.info(f"    Action history: {action_history_shape} = {action_history_size:,} bytes")
        logger.info(f"    Action history: {memory_analysis[format_name]['action_history_mb']:.3f} MB")
        logger.info(f"    Total state: {memory_analysis[format_name]['total_memory_mb']:.3f} MB")
        
        # Show action history percentage of total memory
        action_percentage = (action_history_size / total_memory) * 100
        logger.info(f"    Action history: {action_percentage:.1f}% of total state memory")
    
    # Calculate memory savings
    logger.info("  üìä Memory Savings Analysis:")
    
    mask_memory = memory_analysis['mask']['action_history_mb']
    point_memory = memory_analysis['point']['action_history_mb']
    bbox_memory = memory_analysis['bbox']['action_history_mb']
    
    if mask_memory > 0:
        point_savings = (1 - point_memory / mask_memory) * 100
        bbox_savings = (1 - bbox_memory / mask_memory) * 100
        
        logger.info(f"    Point vs Mask: {point_savings:.1f}% memory reduction")
        logger.info(f"    Bbox vs Mask: {bbox_savings:.1f}% memory reduction")
        logger.info(f"    Point memory: {point_memory:.6f} MB")
        logger.info(f"    Bbox memory: {bbox_memory:.6f} MB")
        logger.info(f"    Mask memory: {mask_memory:.3f} MB")
    
    return memory_analysis


def demonstrate_action_history_efficiency():
    """Demonstrate how action history scales with different formats."""
    logger.info("üìà Action History Efficiency Demo")
    
    # Setup
    parser = ArcAgiParser()
    task = parser.get_task_by_index(0)
    key = jax.random.PRNGKey(42)
    
    # Test different history lengths
    history_lengths = [100, 500, 1000, 5000, 10000]
    formats = ['point', 'bbox', 'mask']
    
    results = {}
    
    for format_name in formats:
        logger.info(f"  Testing {format_name} format...")
        results[format_name] = {}
        
        # Create config for this format
        config = create_test_config()
        config = eqx.tree_at(
            lambda c: c.action.selection_format,
            config,
            format_name
        )
        
        for history_length in history_lengths:
            # Update max history length in config
            config = eqx.tree_at(
                lambda c: c.environment.max_episode_steps,
                config,
                history_length
            )
            
            # Create state
            state, obs = arc_reset(key, config, task)
            
            # Calculate memory for this configuration
            action_history_memory = state.action_history.nbytes
            memory_mb = action_history_memory / (1024 * 1024)
            
            results[format_name][history_length] = {
                'memory_bytes': action_history_memory,
                'memory_mb': memory_mb,
                'memory_per_action': action_history_memory / history_length
            }
            
            logger.info(f"    Length {history_length}: {memory_mb:.3f} MB "
                       f"({results[format_name][history_length]['memory_per_action']:.1f} bytes/action)")
    
    # Analyze scaling
    logger.info("  üìä Memory Scaling Analysis:")
    
    for format_name in formats:
        logger.info(f"    {format_name.capitalize()} format:")
        base_memory = results[format_name][history_lengths[0]]['memory_mb']
        max_memory = results[format_name][history_lengths[-1]]['memory_mb']
        scaling_factor = max_memory / base_memory
        theoretical_scaling = history_lengths[-1] / history_lengths[0]
        
        logger.info(f"      {base_memory:.3f} MB ‚Üí {max_memory:.3f} MB")
        logger.info(f"      Scaling: {scaling_factor:.1f}x (theoretical: {theoretical_scaling:.1f}x)")
        logger.info(f"      Efficiency: {(scaling_factor/theoretical_scaling)*100:.1f}%")
    
    return results


def demonstrate_serialization_efficiency():
    """Demonstrate efficient serialization with task_data exclusion."""
    logger.info("üíæ Serialization Efficiency Demo")
    
    # Setup
    config = create_test_config()
    parser = ArcAgiParser()
    task = parser.get_task_by_index(0)
    key = jax.random.PRNGKey(42)
    
    # Create state
    state, obs = arc_reset(key, config, task)
    
    # Run a few steps to populate action history
    for i in range(10):
        action = PointAction(
            operation=jnp.array(0),
            row=jnp.array(i % 10 + 5),
            col=jnp.array(5)
        )
        state, obs, reward, done, info = arc_step(state, action, config)
    
    logger.info("  Testing serialization methods...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # Method 1: Standard serialization (includes everything)
        standard_path = os.path.join(temp_dir, "standard_state.eqx")
        
        logger.info("    Standard serialization (includes task_data)...")
        start_time = time.perf_counter()
        eqx.tree_serialise_leaves(standard_path, state)
        standard_time = time.perf_counter() - start_time
        standard_size = os.path.getsize(standard_path)
        
        logger.info(f"      Time: {standard_time*1000:.2f}ms")
        logger.info(f"      Size: {standard_size:,} bytes ({standard_size/(1024*1024):.3f} MB)")
        
        # Method 2: Efficient serialization (excludes task_data)
        efficient_path = os.path.join(temp_dir, "efficient_state.eqx")
        
        logger.info("    Efficient serialization (excludes task_data)...")
        
        def efficient_filter_spec(f, x):
            """Custom filter that excludes task_data field."""
            if eqx.is_array(x) or isinstance(x, (int, float, bool, jnp.integer, jnp.floating)):
                return eqx.default_serialise_filter_spec(f, x)
            # Skip non-array fields (like task_data)
            return False
        
        start_time = time.perf_counter()
        eqx.tree_serialise_leaves(efficient_path, state, filter_spec=efficient_filter_spec)
        efficient_time = time.perf_counter() - start_time
        efficient_size = os.path.getsize(efficient_path)
        
        logger.info(f"      Time: {efficient_time*1000:.2f}ms")
        logger.info(f"      Size: {efficient_size:,} bytes ({efficient_size/(1024*1024):.3f} MB)")
        
        # Calculate savings
        size_reduction = (1 - efficient_size / standard_size) * 100
        time_improvement = (1 - efficient_time / standard_time) * 100
        
        logger.info("  üìä Serialization Efficiency:")
        logger.info(f"    Size reduction: {size_reduction:.1f}%")
        logger.info(f"    Time improvement: {time_improvement:.1f}%")
        logger.info(f"    Compression ratio: {standard_size/efficient_size:.1f}:1")
        
        # Test deserialization
        logger.info("  Testing deserialization...")
        
        # Standard deserialization
        start_time = time.perf_counter()
        loaded_standard = eqx.tree_deserialise_leaves(standard_path, state)
        standard_load_time = time.perf_counter() - start_time
        
        # Efficient deserialization (would need task reconstruction in practice)
        start_time = time.perf_counter()
        # Create dummy state for loading
        dummy_state = eqx.tree_at(lambda s: s.task_data, state, None)
        loaded_efficient = eqx.tree_deserialise_leaves(efficient_path, dummy_state)
        efficient_load_time = time.perf_counter() - start_time
        
        logger.info(f"    Standard load time: {standard_load_time*1000:.2f}ms")
        logger.info(f"    Efficient load time: {efficient_load_time*1000:.2f}ms")
        
        return {
            'standard_size': standard_size,
            'efficient_size': efficient_size,
            'size_reduction': size_reduction,
            'standard_time': standard_time,
            'efficient_time': efficient_time,
            'time_improvement': time_improvement
        }


def demonstrate_batch_memory_scaling():
    """Demonstrate how memory scales with batch processing."""
    logger.info("üîÑ Batch Memory Scaling Demo")
    
    # Setup
    config = create_test_config()
    parser = ArcAgiParser()
    task = parser.get_task_by_index(0)
    
    batch_sizes = [1, 4, 16, 64, 256]
    memory_results = {}
    
    for batch_size in batch_sizes:
        logger.info(f"  Testing batch size {batch_size}...")
        
        # Create batch keys
        keys = jax.random.split(jax.random.PRNGKey(42), batch_size)
        
        # Import batch functions
        from jaxarc.envs.functional import batch_reset
        
        # Create batch states
        batch_states, batch_obs = batch_reset(keys, config, task)
        
        # Calculate memory usage
        states_memory = batch_states.working_grid.nbytes
        obs_memory = batch_obs.nbytes
        
        # Estimate action history memory (if populated)
        action_history_memory = batch_states.action_history.nbytes
        
        total_memory = states_memory + obs_memory + action_history_memory
        per_env_memory = total_memory / batch_size
        
        memory_results[batch_size] = {
            'total_memory_mb': total_memory / (1024 * 1024),
            'per_env_memory_mb': per_env_memory / (1024 * 1024),
            'states_memory_mb': states_memory / (1024 * 1024),
            'obs_memory_mb': obs_memory / (1024 * 1024),
            'action_history_memory_mb': action_history_memory / (1024 * 1024)
        }
        
        logger.info(f"    Total memory: {memory_results[batch_size]['total_memory_mb']:.2f} MB")
        logger.info(f"    Per environment: {memory_results[batch_size]['per_env_memory_mb']:.3f} MB")
        logger.info(f"    Memory breakdown:")
        logger.info(f"      States: {memory_results[batch_size]['states_memory_mb']:.2f} MB")
        logger.info(f"      Observations: {memory_results[batch_size]['obs_memory_mb']:.2f} MB")
        logger.info(f"      Action history: {memory_results[batch_size]['action_history_memory_mb']:.2f} MB")
    
    # Analyze scaling efficiency
    logger.info("  üìä Memory Scaling Analysis:")
    
    single_env_memory = memory_results[1]['per_env_memory_mb']
    
    for batch_size in batch_sizes[1:]:
        current_per_env = memory_results[batch_size]['per_env_memory_mb']
        overhead_percent = (current_per_env / single_env_memory - 1) * 100
        efficiency = single_env_memory / current_per_env * 100
        
        logger.info(f"    Batch {batch_size}: {overhead_percent:+.1f}% overhead, "
                   f"{efficiency:.1f}% efficiency")
    
    return memory_results


def provide_memory_optimization_tips():
    """Provide practical memory optimization tips."""
    logger.info("üí° Memory Optimization Best Practices")
    
    tips = [
        {
            'title': "Choose the Right Action Format",
            'description': "Use point actions for single-cell operations, bbox for rectangular regions, mask only when necessary",
            'savings': "Up to 99%+ memory reduction"
        },
        {
            'title': "Optimize Action History Length",
            'description': "Set max_episode_steps to the minimum required for your use case",
            'savings': "Linear memory reduction"
        },
        {
            'title': "Use Efficient Serialization",
            'description': "Exclude large static fields like task_data during serialization",
            'savings': "90%+ file size reduction"
        },
        {
            'title': "Batch Processing Sweet Spot",
            'description': "Find optimal batch size that maximizes throughput without memory issues",
            'savings': "Better memory efficiency per environment"
        },
        {
            'title': "JIT Compilation",
            'description': "Use @eqx.filter_jit to reduce memory allocations during execution",
            'savings': "Reduced temporary memory usage"
        },
        {
            'title': "State Field Management",
            'description': "Only keep necessary fields in state, use lazy loading for large data",
            'savings': "Proportional to unused fields"
        }
    ]
    
    for i, tip in enumerate(tips, 1):
        logger.info(f"  {i}. {tip['title']}")
        logger.info(f"     {tip['description']}")
        logger.info(f"     üíæ Potential savings: {tip['savings']}")
    
    # Provide code examples
    logger.info("\n  üìù Code Examples:")
    
    logger.info("    # Use point actions for memory efficiency")
    logger.info("    action = PointAction(operation=jnp.array(0), row=jnp.array(5), col=jnp.array(5))")
    
    logger.info("\n    # Configure smaller action history")
    logger.info("    config = eqx.tree_at(lambda c: c.environment.max_episode_steps, config, 100)")
    
    logger.info("\n    # Efficient serialization")
    logger.info("    def efficient_filter(f, x):")
    logger.info("        return eqx.is_array(x) or isinstance(x, (int, float, bool))")
    logger.info("    eqx.tree_serialise_leaves(path, state, filter_spec=efficient_filter)")
    
    return tips


def main():
    """Run all memory efficiency demonstrations."""
    logger.info("üíæ JaxARC Memory Efficiency Demonstration")
    logger.info("=" * 60)
    
    try:
        # Run demonstrations
        format_analysis = analyze_action_format_memory()
        history_analysis = demonstrate_action_history_efficiency()
        serialization_analysis = demonstrate_serialization_efficiency()
        batch_analysis = demonstrate_batch_memory_scaling()
        optimization_tips = provide_memory_optimization_tips()
        
        # Summary
        logger.info("\nüéâ Memory Efficiency Summary")
        logger.info("=" * 60)
        
        # Calculate key metrics
        mask_memory = format_analysis['mask']['action_history_mb']
        point_memory = format_analysis['point']['action_history_mb']
        
        if mask_memory > 0:
            memory_reduction = (1 - point_memory / mask_memory) * 100
        else:
            memory_reduction = 0
        
        size_reduction = serialization_analysis['size_reduction']
        
        logger.info(f"‚úÖ Action format optimization: {memory_reduction:.1f}% memory reduction")
        logger.info(f"‚úÖ Serialization optimization: {size_reduction:.1f}% file size reduction")
        logger.info(f"‚úÖ Batch processing: Linear scaling with minimal overhead")
        logger.info(f"‚úÖ Best practices: {len(optimization_tips)} optimization strategies")
        
        logger.info(f"\nüöÄ Memory optimizations can reduce usage by 90%+ in many cases!")
        
    except Exception as e:
        logger.error(f"‚ùå Error during demonstration: {e}")
        raise


if __name__ == "__main__":
    main()