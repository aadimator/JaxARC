{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23741476",
   "metadata": {},
   "source": [
    "# Creating Agents for ARC Tasks\n",
    "\n",
    "In JaxARC, an agent is simply a function that takes an observation and returns an action. The environment uses the `TimeStep` pattern where:\n",
    "- `state` contains the internal environment state\n",
    "- `timestep` contains observations, rewards, and termination flags\n",
    "- Actions are sampled from the environment's action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fffcd7",
   "metadata": {},
   "source": [
    "## Setup: Create an Environment\n",
    "\n",
    "First, let's create a JaxARC environment. We'll use a simple configuration optimized for agent development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db082d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "\n",
    "from jaxarc.configs import JaxArcConfig\n",
    "from jaxarc.registration import available_task_ids, make\n",
    "from jaxarc.utils.core import get_config\n",
    "\n",
    "# Configure environment with visualization and logging disabled for speed\n",
    "config_overrides = [\n",
    "    \"dataset=mini_arc\",\n",
    "    \"action=raw\",\n",
    "    \"wandb.enabled=false\",\n",
    "    \"logging.log_operations=false\",\n",
    "    \"logging.log_rewards=false\",\n",
    "    \"visualization.enabled=false\",\n",
    "]\n",
    "\n",
    "# Load configuration\n",
    "hydra_config = get_config(overrides=config_overrides)\n",
    "config = JaxArcConfig.from_hydra(hydra_config)\n",
    "\n",
    "# Get a task from MiniARC\n",
    "available_ids = available_task_ids(\"Mini\", config=config, auto_download=False)\n",
    "task_id = available_ids[0]\n",
    "\n",
    "# Create environment\n",
    "env, env_params = make(f\"Mini-{task_id}\", config=config)\n",
    "\n",
    "print(f\"Environment created for task: {task_id}\")\n",
    "print(f\"Action space: {env.action_space(env_params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5acdb3",
   "metadata": {},
   "source": [
    "## Understanding the Environment Loop\n",
    "\n",
    "Before creating an agent, let's understand how to interact with the environment using the TimeStep API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment\n",
    "key = jr.PRNGKey(42)\n",
    "state, timestep = env.reset(key, env_params)\n",
    "\n",
    "print(f\"Observation shape: {timestep.observation.shape}\")\n",
    "print(f\"Initial reward: {timestep.reward}\")\n",
    "print(f\"Episode terminated: {timestep.last()}\")\n",
    "\n",
    "# Get action space for sampling\n",
    "action_space = env.action_space(env_params)\n",
    "\n",
    "# Sample and take a single action\n",
    "key, action_key = jr.split(key)\n",
    "action = action_space.sample(action_key)\n",
    "\n",
    "print(f\"\\nSampled action: {action}\")\n",
    "\n",
    "# Step the environment\n",
    "state, timestep = env.step(state, action, env_params)\n",
    "\n",
    "print(\"\\nAfter step:\")\n",
    "print(f\"Reward: {timestep.reward}\")\n",
    "print(f\"Episode done: {timestep.last()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c198f6",
   "metadata": {},
   "source": [
    "## Creating a Random Agent\n",
    "\n",
    "The simplest agent samples random actions from the action space. This serves as a baseline for comparison with more sophisticated agents.\n",
    "\n",
    "### Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b8e3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single episode with a random agent\n",
    "def run_random_episode(env, env_params, key, max_steps=100):\n",
    "    \"\"\"Run one episode with random actions.\"\"\"\n",
    "    # Reset environment\n",
    "    reset_key, loop_key = jr.split(key)\n",
    "    state, timestep = env.reset(reset_key, env_params)\n",
    "\n",
    "    action_space = env.action_space(env_params)\n",
    "    episode_reward = 0.0\n",
    "    step_count = 0\n",
    "\n",
    "    # Run episode\n",
    "    while not timestep.last() and step_count < max_steps:\n",
    "        # Sample random action\n",
    "        loop_key, action_key = jr.split(loop_key)\n",
    "        action = action_space.sample(action_key)\n",
    "\n",
    "        # Step environment\n",
    "        state, timestep = env.step(state, action, env_params)\n",
    "\n",
    "        episode_reward += float(timestep.reward)\n",
    "        step_count += 1\n",
    "\n",
    "    return episode_reward, step_count\n",
    "\n",
    "\n",
    "# Test the agent\n",
    "key = jr.PRNGKey(123)\n",
    "reward, steps = run_random_episode(env, env_params, key, max_steps=50)\n",
    "\n",
    "print(\"Episode completed!\")\n",
    "print(f\"Total reward: {reward:.2f}\")\n",
    "print(f\"Steps taken: {steps}\")\n",
    "print(f\"Average reward per step: {reward / steps:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e6938d",
   "metadata": {},
   "source": [
    "## JAX-Accelerated Agent with Scan\n",
    "\n",
    "For high-performance, we can use `jax.lax.scan` to run multiple steps efficiently. This pattern is used in PureJaxRL and similar high-throughput RL frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73acdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_jax_agent(env, env_params, num_steps):\n",
    "    \"\"\"Create a JIT-compiled agent using scan for efficiency.\"\"\"\n",
    "    action_space = env.action_space(env_params)\n",
    "\n",
    "    def run_agent(key):\n",
    "        # Reset environment\n",
    "        reset_key, loop_key = jr.split(key)\n",
    "        state, timestep = env.reset(reset_key, env_params)\n",
    "\n",
    "        def step_fn(carry, _):\n",
    "            \"\"\"One step of the agent.\"\"\"\n",
    "            state, timestep, key = carry\n",
    "\n",
    "            # Split key for action sampling and next iteration\n",
    "            key, action_key, next_key = jr.split(key, 3)\n",
    "\n",
    "            # Handle episode termination with conditional reset\n",
    "            def do_reset(_):\n",
    "                return env.reset(key, env_params)\n",
    "\n",
    "            def continue_episode(_):\n",
    "                return state, timestep\n",
    "\n",
    "            state, timestep = jax.lax.cond(\n",
    "                timestep.last(), do_reset, continue_episode, None\n",
    "            )\n",
    "\n",
    "            # Sample action and step\n",
    "            action = action_space.sample(action_key)\n",
    "            new_state, new_timestep = env.step(state, action, env_params)\n",
    "\n",
    "            return (new_state, new_timestep, next_key), new_timestep.reward\n",
    "\n",
    "        # Run scan over num_steps\n",
    "        (final_state, final_timestep, _), rewards = jax.lax.scan(\n",
    "            step_fn, (state, timestep, loop_key), None, length=num_steps\n",
    "        )\n",
    "\n",
    "        return rewards, final_timestep\n",
    "\n",
    "    # JIT compile the entire function\n",
    "    return jax.jit(run_agent)\n",
    "\n",
    "\n",
    "# Create and run JIT-compiled agent\n",
    "jax_agent = make_jax_agent(env, env_params, num_steps=100)\n",
    "\n",
    "# First run includes compilation time\n",
    "print(\"Compiling agent (first run)...\")\n",
    "key = jr.PRNGKey(456)\n",
    "rewards, final_timestep = jax_agent(key)\n",
    "\n",
    "print(\"Agent compiled and executed!\")\n",
    "print(f\"Total reward: {float(rewards.sum()):.2f}\")\n",
    "print(f\"Mean reward per step: {float(rewards.mean()):.3f}\")\n",
    "print(f\"Max reward: {float(rewards.max()):.2f}\")\n",
    "print(f\"Final episode terminated: {final_timestep.last()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a51ac2",
   "metadata": {},
   "source": [
    "## Vectorized Agent: Multiple Parallel Environments\n",
    "\n",
    "JAX's `vmap` allows us to run multiple environments in parallel with a single function call. This dramatically increases throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a9b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vectorized_agent(env, env_params, num_envs, num_steps):\n",
    "    \"\"\"Create a vectorized agent that runs multiple environments in parallel.\"\"\"\n",
    "\n",
    "    # Create the single-env agent\n",
    "    single_agent = make_jax_agent(env, env_params, num_steps)\n",
    "\n",
    "    # Vectorize it across multiple environments\n",
    "    vectorized_agent = jax.vmap(single_agent)\n",
    "\n",
    "    return vectorized_agent\n",
    "\n",
    "\n",
    "# Create vectorized agent\n",
    "num_envs = 16\n",
    "num_steps = 100\n",
    "\n",
    "print(f\"Creating vectorized agent with {num_envs} parallel environments...\")\n",
    "vec_agent = make_vectorized_agent(env, env_params, num_envs, num_steps)\n",
    "\n",
    "# Generate keys for each environment\n",
    "key = jr.PRNGKey(789)\n",
    "env_keys = jr.split(key, num_envs)\n",
    "\n",
    "# Run all environments in parallel\n",
    "print(f\"Running {num_envs} environments × {num_steps} steps...\")\n",
    "all_rewards, all_final_timesteps = vec_agent(env_keys)\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\nResults across {num_envs} environments:\")\n",
    "print(f\"Mean total reward: {float(all_rewards.sum(axis=1).mean()):.2f}\")\n",
    "print(f\"Best environment reward: {float(all_rewards.sum(axis=1).max()):.2f}\")\n",
    "print(f\"Worst environment reward: {float(all_rewards.sum(axis=1).min()):.2f}\")\n",
    "print(f\"Mean reward per step: {float(all_rewards.mean()):.3f}\")\n",
    "\n",
    "# Total steps executed\n",
    "total_steps = num_envs * num_steps\n",
    "print(f\"\\nTotal steps executed: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632fc1e",
   "metadata": {},
   "source": [
    "## Performance Benchmark\n",
    "\n",
    "Let's measure the throughput of our vectorized agent to understand the performance benefits of JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Benchmark configuration\n",
    "num_envs = 64\n",
    "num_steps = 256\n",
    "num_runs = 3\n",
    "\n",
    "print(\"Benchmarking vectorized agent...\")\n",
    "print(f\"Configuration: {num_envs} envs × {num_steps} steps\")\n",
    "print(\"Warmup run (includes compilation)...\\n\")\n",
    "\n",
    "# Create fresh agent\n",
    "vec_agent = make_vectorized_agent(env, env_params, num_envs, num_steps)\n",
    "key = jr.PRNGKey(999)\n",
    "env_keys = jr.split(key, num_envs)\n",
    "\n",
    "# Warmup run (includes compilation)\n",
    "start = time.time()\n",
    "rewards, _ = vec_agent(env_keys)\n",
    "_ = rewards.block_until_ready()  # Wait for computation\n",
    "warmup_time = time.time() - start\n",
    "\n",
    "print(f\"Warmup complete: {warmup_time:.2f}s (includes JIT compilation)\")\n",
    "\n",
    "# Timed runs\n",
    "print(f\"\\nRunning {num_runs} timed iterations...\")\n",
    "times = []\n",
    "\n",
    "for i in range(num_runs):\n",
    "    key, subkey = jr.split(key)\n",
    "    env_keys = jr.split(subkey, num_envs)\n",
    "\n",
    "    start = time.time()\n",
    "    rewards, _ = vec_agent(env_keys)\n",
    "    _ = rewards.block_until_ready()\n",
    "    elapsed = time.time() - start\n",
    "    times.append(elapsed)\n",
    "\n",
    "    print(f\"  Run {i + 1}: {elapsed:.3f}s\")\n",
    "\n",
    "# Calculate statistics\n",
    "mean_time = sum(times) / len(times)\n",
    "total_steps = num_envs * num_steps\n",
    "sps = total_steps / mean_time\n",
    "\n",
    "print(\"\\nPerformance Results:\")\n",
    "print(f\"Mean execution time: {mean_time:.3f}s\")\n",
    "print(f\"Steps per second (SPS): {sps:,.0f}\")\n",
    "print(f\"Total steps per run: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0829e4f8",
   "metadata": {},
   "source": [
    "## Building Your Own Agent\n",
    "\n",
    "To create a learning agent (not just random):\n",
    "\n",
    "1. **Define a neural network** using Flax, Haiku, or Equinox\n",
    "2. **Collect trajectories** using the scan pattern shown above\n",
    "3. **Compute losses** from rewards and observations\n",
    "4. **Update parameters** using Optax optimizers\n",
    "5. **Repeat** the training loop"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
