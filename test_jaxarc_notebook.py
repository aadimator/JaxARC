# %% [markdown]
# # JaxARC Environment Testing Notebook
#
# This notebook tests the JaxARC environment with:
# - Mini ARC dataset (5x5 grids for fast testing)
# - Raw action format (minimal operations: fill colors 0-9, resize, submit)
# - Verification of all actions working correctly
# - Simple random agent testing the RL loop

# %%
from __future__ import annotations

from pathlib import Path

import jax
import jax.numpy as jnp
import jax.random as jr
from rich.console import Console
from rich.panel import Panel
from loguru import logger
from omegaconf import OmegaConf

from jaxarc.envs import ArcEnvironment

# Import JaxARC components
from jaxarc.parsers import MiniArcParser
from jaxarc.types import JaxArcTask
from jaxarc.utils.config import get_config
from jaxarc.utils.task_manager import create_jax_task_index
from jaxarc.utils.visualization import draw_grid_svg, log_grid_to_console

console = Console()

# %% [markdown]
# ## 1. Environment Setup
#
# Let's create the environment with MiniARC config and raw actions.

# %%
console.print(
    Panel.fit(
        "[bold blue]üîß Setting up JaxARC Environment[/bold blue]", border_style="blue"
    )
)

config = get_config(
    overrides=[
        'dataset=mini_arc', 
        'debug="on"',
        'action=raw', 
        'action.selection_format=point'
        ]
)
logger.info(f"Using config:\n {OmegaConf.to_yaml(config)}")

parser = MiniArcParser(config.dataset)

# Create environment
env = ArcEnvironment(config)

console.print("‚úÖ Environment created successfully")
console.print(
    f"üìä Grid size: {config.dataset.grid.max_grid_height}x{config.dataset.grid.max_grid_width}"
)
console.print(f"üéØ Action format: {config.action.selection_format}")
console.print(
    f"üîß Allowed operations: {len(config.action.allowed_operations)} operations"
)
console.print(f"‚è±Ô∏è  Max episode steps: {config.environment.max_episode_steps}")

# %% [markdown]
# ## 2. Create Demo Task and Initial Reset
#
# Let's create a demo task and reset the environment.

# %%
console.print(
    Panel.fit(
        "[bold green]üéÆ Creating Demo Task & Environment Reset[/bold green]",
        border_style="green",
    )
)

# Create demo task
key = jr.PRNGKey(42)
demo_task = parser.get_random_task(key=key)

console.print("üìã Demo task created:")
console.print(f"  ‚Ä¢ Input grid shape: {demo_task.test_input_grids.shape}")
console.print(f"  ‚Ä¢ Training pairs: {demo_task.num_train_pairs}")
console.print(f"  ‚Ä¢ Test pairs: {demo_task.num_test_pairs}")

# Reset environment with the demo task
reset_key, key = jr.split(key)
state, observation = env.reset(reset_key, task_data=demo_task)

console.print("‚úÖ Environment reset successful")
console.print(f"üìä Working grid shape: {state.working_grid.shape}")
console.print(f"üéØ Step count: {state.step_count}")

# Display initial grids
console.print("\n[bold cyan]üìã Training Example:[/bold cyan]")
console.print("[dim]Input (5x5 area):[/dim]")
input_5x5 = demo_task.input_grids_examples[0][:5, :5]
console.print(f"Grid values: {input_5x5.tolist()}")
log_grid_to_console(input_5x5)

console.print("[dim]Expected Output (5x5 area):[/dim]")
output_5x5 = demo_task.output_grids_examples[0][:5, :5]
console.print(f"Grid values: {output_5x5.tolist()}")
log_grid_to_console(output_5x5)

console.print("\n[bold yellow]üéØ Current Working Grid (5x5 area):[/bold yellow]")
working_5x5 = state.working_grid[:5, :5]
console.print(f"Grid values: {working_5x5.tolist()}")
log_grid_to_console(working_5x5)

# %% [markdown]
# ## 3. Test All Available Actions
#
# Let's systematically test each allowed operation to verify they work correctly.


# %%
console.print(
    Panel.fit(
        "[bold magenta]üß™ Testing All Available Actions[/bold magenta]",
        border_style="magenta",
    )
)

# Get the allowed operations from config
allowed_ops = config.action.allowed_operations
console.print(f"Testing {len(allowed_ops)} operations: {allowed_ops}")

# Operation names for display
op_names = {
    0: "FILL_0 (black)",
    1: "FILL_1 (blue)",
    2: "FILL_2 (red)",
    3: "FILL_3 (green)",
    4: "FILL_4 (yellow)",
    5: "FILL_5 (gray)",
    6: "FILL_6 (magenta)",
    7: "FILL_7 (orange)",
    8: "FILL_8 (light_blue)",
    9: "FILL_9 (brown)",
    33: "RESIZE",
    34: "SUBMIT",
}

# Test each operation
test_results = []
action_key = key

for _, op_id in enumerate(
    allowed_ops[:5]
):  # Test first 5 operations to keep output manageable
    console.print(
        f"\n[bold]Testing Operation {op_id}: {op_names.get(op_id, f'Unknown_{op_id}')}[/bold]"
    )

    # Create a random point selection within the valid grid area
    actual_height, actual_width = state.get_actual_grid_shape()
    row = jr.randint(action_key, shape=(), minval=0, maxval=actual_height)
    col = jr.randint(action_key, shape=(), minval=0, maxval=actual_width)
    
    action = {
        "point": jnp.array([row, col], dtype=jnp.int32),
        "operation": jnp.array(op_id, dtype=jnp.int32)
    }

    # Store state before action
    grid_before = state.working_grid.copy()

    # Take the action
    action_key, key = jr.split(key)
    try:
        state, observation, reward, info = env.step(action)

        # Check if grid changed
        grid_changed = not jnp.array_equal(grid_before, state.working_grid)

        result = {
            "operation": op_id,
            "name": op_names.get(op_id, f"Unknown_{op_id}"),
            "success": True,
            "grid_changed": grid_changed,
            "reward": float(reward),
            "similarity": float(info.get("similarity", 0.0)),
        }

        console.print(
            f"  ‚úÖ Success! Grid changed: {grid_changed}, Reward: {reward:.3f}"
        )

        if grid_changed and op_id < 10:  # For fill operations, show the change
            console.print("  [dim]Grid after action:[/dim]")
            log_grid_to_console(state.working_grid)

    except Exception as e:
        result = {
            "operation": op_id,
            "name": op_names.get(op_id, f"Unknown_{op_id}"),
            "success": False,
            "error": str(e),
            "reward": 0.0,
        }
        console.print(f"  ‚ùå Failed: {e}")

    test_results.append(result)

# Summary of action tests
console.print(f"\n[bold]üìä Action Test Summary:[/bold]")
successful_ops = [r for r in test_results if r["success"]]
console.print(f"  ‚Ä¢ Successful operations: {len(successful_ops)}/{len(test_results)}")
console.print(
    f"  ‚Ä¢ Operations that changed grid: {len([r for r in successful_ops if r.get('grid_changed', False)])}"
)

# %% [markdown]
# ## 5. Random Agent Testing
#
# Now let's test a simple random agent to verify the RL loop works correctly.

# %%
console.print(
    Panel.fit("[bold red]ü§ñ Random Agent Testing[/bold red]", border_style="red")
)


def create_random_action(
    key: jr.PRNGKey, allowed_ops: list, 
    grid_height: int = 5, grid_width: int = 5
) -> dict:
    """Create a random action using allowed operations with point format."""
    op_key, point_key = jr.split(key)

    # Random operation from allowed set
    op_idx = jr.randint(op_key, shape=(), minval=0, maxval=len(allowed_ops))
    operation = jnp.array(allowed_ops[int(op_idx)], dtype=jnp.int32)

    # Random point within the grid
    row = jr.randint(point_key, shape=(), minval=0, maxval=grid_height)
    col = jr.randint(point_key, shape=(), minval=0, maxval=grid_width)
    point = jnp.array([row, col], dtype=jnp.int32)

    return {"point": point, "operation": operation}


# Run random agent for multiple episodes
num_episodes = 3
episode_results = []

for episode in range(num_episodes):
    console.print(f"\n[bold]üéÆ Episode {episode + 1}/{num_episodes}[/bold]")

    # Reset environment
    reset_key, key = jr.split(key)
    state, observation = env.reset(reset_key, task_data=demo_task)

    episode_reward = 0.0
    episode_steps = 0
    step_rewards = []

    # Run episode
    while not env.is_done and episode_steps < config.environment.max_episode_steps:
        # Create random action
        action_key, key = jr.split(key)
        actual_height, actual_width = state.get_actual_grid_shape()
        action = create_random_action(
            action_key, allowed_ops[:-2], actual_height, actual_width
        )

        logger.info(action)

        # Take step
        state, observation, reward, info = env.step(action)

        episode_reward += reward
        episode_steps += 1
        step_rewards.append(float(reward))

        # Log every 10 steps
        if episode_steps % 10 == 0:
            console.print(
                f"  Step {episode_steps}: reward={reward:.3f}, similarity={info.get('similarity', 0):.3f}"
            )

    # Episode summary
    final_similarity = info.get("similarity", 0.0)
    episode_result = {
        "episode": episode + 1,
        "total_reward": episode_reward,
        "steps": episode_steps,
        "final_similarity": final_similarity,
        "avg_reward_per_step": episode_reward / max(episode_steps, 1),
        "completed": env.is_done,
    }
    episode_results.append(episode_result)

    console.print(f"  üìä Episode {episode + 1} Summary:")
    console.print(f"    ‚Ä¢ Total reward: {episode_reward:.3f}")
    console.print(f"    ‚Ä¢ Steps taken: {episode_steps}")
    console.print(f"    ‚Ä¢ Final similarity: {final_similarity:.3f}")
    console.print(
        f"    ‚Ä¢ Avg reward/step: {episode_reward / max(episode_steps, 1):.3f}"
    )
    console.print(f"    ‚Ä¢ Episode completed: {'‚úÖ Yes' if env.is_done else '‚ùå No'}")

# %% [markdown]
# ## 6. Results Summary
#
# Let's summarize all our testing results.

# %%
console.print(
    Panel.fit(
        "[bold green]üìä Testing Results Summary[/bold green]", border_style="green"
    )
)

# Action testing summary
console.print("[bold]üß™ Action Testing Results:[/bold]")
successful_actions = len([r for r in test_results if r["success"]])
console.print(f"  ‚Ä¢ Actions tested: {len(test_results)}")
console.print(f"  ‚Ä¢ Successful actions: {successful_actions}")
console.print(f"  ‚Ä¢ Success rate: {successful_actions / len(test_results) * 100:.1f}%")

# Random agent summary
console.print(f"\n[bold]ü§ñ Random Agent Results ({num_episodes} episodes):[/bold]")
total_steps = sum(r["steps"] for r in episode_results)
total_reward = sum(r["total_reward"] for r in episode_results)
avg_similarity = sum(r["final_similarity"] for r in episode_results) / len(
    episode_results
)

console.print(f"  ‚Ä¢ Total steps across all episodes: {total_steps}")
console.print(f"  ‚Ä¢ Total reward across all episodes: {total_reward:.3f}")
console.print(f"  ‚Ä¢ Average steps per episode: {total_steps / num_episodes:.1f}")
console.print(f"  ‚Ä¢ Average reward per episode: {total_reward / num_episodes:.3f}")
console.print(f"  ‚Ä¢ Average final similarity: {avg_similarity:.3f}")

# Environment verification
console.print(f"\n[bold]‚úÖ Environment Verification:[/bold]")
console.print(f"  ‚Ä¢ Environment setup: ‚úÖ Success")
console.print(f"  ‚Ä¢ Task loading: ‚úÖ Success")
console.print(f"  ‚Ä¢ Action execution: ‚úÖ Success")
console.print(f"  ‚Ä¢ Reward calculation: ‚úÖ Success")
console.print(f"  ‚Ä¢ Episode management: ‚úÖ Success")
console.print(f"  ‚Ä¢ RL loop functionality: ‚úÖ Success")

console.print(f"\n[bold cyan]üéâ All tests completed successfully![/bold cyan]")
console.print(
    f"The JaxARC environment with MiniARC dataset and raw actions is working correctly."
)

# %%
